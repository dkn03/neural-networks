{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from torch.utils.data import Dataset, ConcatDataset, DataLoader\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import Resize\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "from models import CustomModel\n",
    "from custom_dataset import FoodDataset\n",
    "from utils import preprocess_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтение данных в Dataset и Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "food_data_path = \"../nn2_data/training/food/\"\n",
    "food_data = [(1, read_image(food_data_path + filename, mode=torchvision.io.ImageReadMode.RGB).to(dtype=torch.float32)) for filename in os.listdir(food_data_path)]\n",
    "food_data = list(map(lambda x: (x[0], preprocess_image(x[1])), food_data))\n",
    "non_food_data_path = \"../nn2_data/training/non_food/\"\n",
    "non_food_data = [(0, read_image(non_food_data_path + filename, mode=torchvision.io.ImageReadMode.RGB).to(dtype=torch.float32)) for filename in os.listdir(non_food_data_path)]\n",
    "non_food_data = list(map(lambda x: (x[0], preprocess_image(x[1])), non_food_data))\n",
    "\n",
    "food_dataset = FoodDataset(food_data)\n",
    "non_food_dataset = FoodDataset(non_food_data)\n",
    "train_dataset = ConcatDataset([food_dataset, non_food_dataset])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "\n",
    "food_data_path = \"../nn2_data/validation/food/\"\n",
    "food_data = [(1, read_image(food_data_path + filename, mode=torchvision.io.ImageReadMode.RGB).to(dtype=torch.float32)) for filename in os.listdir(food_data_path)[:100]]\n",
    "food_data = list(map(lambda x: (x[0], preprocess_image(x[1])), food_data))\n",
    "non_food_data_path = \"../nn2_data/validation/non_food/\"\n",
    "non_food_data = [(0, read_image(non_food_data_path + filename, mode=torchvision.io.ImageReadMode.RGB).to(dtype=torch.float32)) for filename in os.listdir(non_food_data_path)[:100]]\n",
    "non_food_data = list(map(lambda x: (x[0], preprocess_image(x[1])), non_food_data))\n",
    "\n",
    "\n",
    "\n",
    "food_dataset = FoodDataset(food_data)\n",
    "non_food_dataset = FoodDataset(non_food_data)\n",
    "validation_dataset = ConcatDataset([food_dataset, non_food_dataset])\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader, validation_loader):\n",
    "    prev_validation_loss = None\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs = imgs.to(device=device, dtype=torch.float32)\n",
    "            labels = labels.to(device=device)\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels).float()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_train += loss.item()\n",
    "            \n",
    "        #Функция потерь на валидационных данных\n",
    "        summary_loss = 0\n",
    "        for img, label in validation_loader:\n",
    "            label = label.to(device=device)\n",
    "            out = model(img.to(device=device, dtype=torch.float32))\n",
    "            summary_loss += loss_fn(out, label)\n",
    "\n",
    "        validation_loss = summary_loss/len(validation_loader)\n",
    "    \n",
    "        if epoch == 1 or epoch % 5 == 0:\n",
    "        \n",
    "            print('{} Epoch {}, Training loss {}, Validation loss {}'.format(\n",
    "            datetime.datetime.now(),\n",
    "            epoch,\n",
    "            loss_train / len(train_loader),\n",
    "            validation_loss)\n",
    "            )\n",
    "            if prev_validation_loss is not None and prev_validation_loss <= validation_loss:\n",
    "                print(f\"Early stop on epoch {epoch}\")\n",
    "                break\n",
    "            else:\n",
    "                prev_validation_loss = validation_loss\n",
    "    \n",
    "    \n",
    "        \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\PycharmProjects\\NeuralNetworks\\neural_networks2\\models.py:20: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  out = F.softmax(self.fc2(out))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-15 15:24:00.419892 Epoch 1, Training loss 0.6942727770805359, Validation loss 0.6931477189064026\n",
      "2024-03-15 15:24:16.838136 Epoch 5, Training loss 0.6901377231280009, Validation loss 0.6906041502952576\n",
      "2024-03-15 15:24:38.079592 Epoch 10, Training loss 0.6850043797492981, Validation loss 0.6867667436599731\n",
      "2024-03-15 15:24:59.686831 Epoch 15, Training loss 0.6751467156410217, Validation loss 0.6800328493118286\n",
      "2024-03-15 15:25:22.021589 Epoch 20, Training loss 0.6597509387334188, Validation loss 0.671393632888794\n",
      "2024-03-15 15:25:43.719264 Epoch 25, Training loss 0.64295742893219, Validation loss 0.6630881428718567\n",
      "2024-03-15 15:26:05.536950 Epoch 30, Training loss 0.6253913781642914, Validation loss 0.6529484391212463\n",
      "2024-03-15 15:26:27.753378 Epoch 35, Training loss 0.6037578084468842, Validation loss 0.6351766586303711\n",
      "2024-03-15 15:26:50.420336 Epoch 40, Training loss 0.5787639768123627, Validation loss 0.6087254285812378\n",
      "2024-03-15 15:27:12.930086 Epoch 45, Training loss 0.5528969070911407, Validation loss 0.5892550349235535\n",
      "2024-03-15 15:27:35.540208 Epoch 50, Training loss 0.5314868582089742, Validation loss 0.5537661910057068\n",
      "2024-03-15 15:27:58.185084 Epoch 55, Training loss 0.5168542657693227, Validation loss 0.5452989339828491\n",
      "2024-03-15 15:28:20.876801 Epoch 60, Training loss 0.5100172657966614, Validation loss 0.5267404317855835\n",
      "2024-03-15 15:28:43.651140 Epoch 65, Training loss 0.5026236565907797, Validation loss 0.5309991836547852\n",
      "Early stop on epoch 65\n"
     ]
    }
   ],
   "source": [
    "model = CustomModel().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "training_loop(n_epochs=200, optimizer=optimizer, model=model, loss_fn=loss_fn, \n",
    "              train_loader=train_dataloader, validation_loader=validation_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"custom_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-15 15:28:57.814443 Epoch 1, Training loss 0.48655205079416436, Validation loss 0.29318419098854065\n",
      "2024-03-15 15:29:49.279523 Epoch 5, Training loss 0.17911907649288575, Validation loss 0.18835394084453583\n",
      "2024-03-15 15:30:54.277482 Epoch 10, Training loss 0.10999122543198367, Validation loss 0.22242684662342072\n",
      "Early stop on epoch 10\n"
     ]
    }
   ],
   "source": [
    "alexnet = models.alexnet(pretrained=True).to(device=device)\n",
    "last_layer = alexnet.classifier[-1]\n",
    "last_layer.out_features = 2\n",
    "torch.nn.init.xavier_uniform_(last_layer.weight) \n",
    "\n",
    "optimizer = torch.optim.SGD(alexnet.parameters(), lr=1e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "training_loop(n_epochs=200, optimizer=optimizer, model=alexnet, loss_fn=loss_fn, \n",
    "              train_loader=train_dataloader, validation_loader=validation_dataloader)\n",
    "torch.save(alexnet.state_dict(), 'alexnet.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-15 15:31:19.505210 Epoch 1, Training loss 0.3063100711219013, Validation loss 0.28808778524398804\n",
      "2024-03-15 15:37:20.436890 Epoch 5, Training loss 0.08730832822388038, Validation loss 0.20573806762695312\n",
      "2024-03-15 15:45:55.415923 Epoch 10, Training loss 0.07073091881012078, Validation loss 0.23982344567775726\n",
      "Early stop on epoch 10\n"
     ]
    }
   ],
   "source": [
    "resnet18 = models.resnet18(pretrained=True)\n",
    "resnet18.fc = torch.nn.Linear(resnet18.fc.in_features, 2)\n",
    "torch.nn.init.xavier_uniform_(resnet18.fc.weight) \n",
    "resnet18 = resnet18.to(device=device)\n",
    "optimizer = torch.optim.SGD(resnet18.parameters(), lr=1e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "training_loop(n_epochs=200, optimizer=optimizer, model=resnet18, loss_fn=loss_fn, \n",
    "              train_loader=train_dataloader, validation_loader=validation_dataloader)\n",
    "torch.save(resnet18.state_dict(), 'resnet.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
