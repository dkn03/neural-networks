{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from custom_dataset import FoodDataset\n",
    "from torchvision.io import read_image\n",
    "from torchvision.models import resnet18, alexnet\n",
    "\n",
    "from torch.utils.data import ConcatDataset, DataLoader\n",
    "from models import CustomModel\n",
    "from utils import preprocess_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_data_path = \"../nn2_data/evaluation/food/\"\n",
    "food_data = [(1, read_image(food_data_path + filename, mode=torchvision.io.ImageReadMode.RGB).to(dtype=torch.float32, device=device)) for filename in os.listdir(food_data_path)]\n",
    "#food_data = list(map(lambda x: (x[0], preprocess_image(x[1])), food_data))\n",
    "non_food_data_path = \"../nn2_data/evaluation/non_food/\"\n",
    "non_food_data = [(0, read_image(non_food_data_path + filename, mode=torchvision.io.ImageReadMode.RGB).to(dtype=torch.float32, device=device)) for filename in os.listdir(non_food_data_path)]\n",
    "#non_food_data = list(map(lambda x: (x[0], preprocess_image(x[1])), non_food_data))\n",
    "food_data.extend(non_food_data)\n",
    "food_dataset = FoodDataset(food_data)\n",
    "non_food_dataset = FoodDataset(non_food_data)\n",
    "dataset = ConcatDataset([food_dataset, non_food_dataset])\n",
    "dataloader = DataLoader(dataset, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(model, data, unsqueeze=False):\n",
    "    counter = 0\n",
    "    for label, image in data:\n",
    "        image = preprocess_image(image)\n",
    "        if unsqueeze:\n",
    "            image = image.unsqueeze(0)\n",
    "        predicted = model(image).argmax()\n",
    "        if predicted == label:\n",
    "            counter +=1\n",
    "    return counter/len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "c:\\Users\\admin\\PycharmProjects\\NeuralNetworks\\neural_networks2\\models.py:20: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  out = F.softmax(self.fc2(out))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.787\n"
     ]
    }
   ],
   "source": [
    "custom_model = CustomModel().to(device=device)\n",
    "custom_model.load_state_dict(torch.load(\"custom_model.pt\"))\n",
    "custom_model.eval()\n",
    "print(get_accuracy(custom_model, food_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.959"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_model = resnet18(num_classes=2).to(device=device)\n",
    "resnet_model.load_state_dict(torch.load(\"resnet.pt\"))\n",
    "resnet_model.eval()\n",
    "get_accuracy(resnet_model, food_data, unsqueeze=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.921"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alexnet_model = alexnet().to(device=device)\n",
    "alexnet_model.load_state_dict(torch.load(\"alexnet.pt\"))\n",
    "alexnet_model.eval()\n",
    "get_accuracy(alexnet_model, food_data, unsqueeze=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
